{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Classification from EEG\n",
    "#### Tyrome Sweet and Taran Rallings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following analyes EEG data taken in experiments where participants where exposed to light and sound events. This code cleans that 32 channel EEG data and uses a long short-term memory recurrent neural network to classify the time following light or sound events by which event occured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the random seed for reproducibility\n",
    "import random\n",
    "seed=42\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 33 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bcee8828258c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnew_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meeg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnew_columns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'time'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnew_columns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m33\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sample'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0meeg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 33 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# eeg1 and events1 are the test data from a single person\n",
    "# code assumes eeg1 and events1 are csv files in the current working directory\n",
    "\n",
    "eeg1 = pd.read_csv(\"full_data_shuffle_24_04(2997).csv\", delimiter=\"\\t\")\n",
    "new_columns = eeg1.columns.values \n",
    "new_columns[0] = 'time'     \n",
    "new_columns[33] = 'sample' \n",
    "eeg1.columns = new_columns\n",
    "\n",
    "events1 = pd.read_csv(\"events1.csv\") #, delimiter=\"\\t\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subsample of the data to ease building the model, unused in final run\n",
    "eeg1_smol = eeg1[0:785000]\n",
    "events1_smol = events1[0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Toy data generator, unused in final run\n",
    "\n",
    "def generate_eeg(samples, time_steps, n_features, event_types):\n",
    "    # samples is Int number of trials \n",
    "    # time_steps is Int length of each trial in ms\n",
    "    # n_features is Int number of EEG channels\n",
    "    # event_types is Int number of stimula like lights and flashes\n",
    "    signals = generate_signals(samples, time_steps, n_features)\n",
    "    events = generate_events(event_types, samples)\n",
    "    events_1hot = one_hot_events(events)\n",
    "    return signals, events_1hot\n",
    "\n",
    "# helper function (generate_eeg) for making EEG signal data\n",
    "def generate_signals(samples, time_steps, n_features):\n",
    "    # data types same as main function\n",
    "    signals = np.random.random((samples, time_steps, n_features))\n",
    "    return signals\n",
    "\n",
    "# helper function (generate_eeg) for making one sample per event an\n",
    "def generate_events(event_types, samples):\n",
    "    # data types same as main function\n",
    "    events = np.random.randint(1, event_types, samples)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in eeg dataframe and event dataframe, cleans them, 1hot encodes the events\n",
    "def clean_eeg(eeg, events, event_interval_length, eeg_slice_length):\n",
    "    #event_list = []\n",
    "    array_list = [] \n",
    "    index_list = []\n",
    "    eeg = standardize_eeg(eeg) # function for standardizing the eeg readings\n",
    "    #events_new = build_zero_events(events)\n",
    "    # iterate over the rows of the events and slice out the corresponding eeg data\n",
    "    for index, row in itertools.islice(events.iterrows(), event_interval_length): # loop through events data\n",
    "        #build_event_list(row, event_list) #\n",
    "        tmin, tmax = build_event_intervals(row, events)\n",
    "        eeg_slice = cut_event_intervals(eeg, tmin, tmax)\n",
    "        array_list, index_list = build_array(eeg_slice, eeg_slice_length, \n",
    "                                             index, index_list, array_list)\n",
    "    y_int = events.iloc[index_list] # take the event types for the correct index\n",
    "    y_int = y_int['type'].values    # take just the event types as an array\n",
    "    #y_int = y_int.as_matrix()            # save the event types as a matrix\n",
    "    #y, lb = one_hot_events(y_int)        # one-hot the event types and save the binarizer\n",
    "    X = np.stack(array_list, axis = 0)   # stack the arrays so the whole thing is 3D\n",
    "    return X, y_int                     # return the data, outputs, and the binarizer\n",
    "    \n",
    "        \n",
    "def build_event_list(row, event_list):\n",
    "    # helper function to pull event types out of event data in the right order\n",
    "    event_type = getattr(row, \"type\")\n",
    "    event_list.append(event_type)\n",
    "        \n",
    "def build_event_intervals(row, events):\n",
    "    # helper function to get the time intervals associated with each event\n",
    "    tmin = getattr(row, \"latency\")\n",
    "    tmin_in = getattr(row, \"number\")\n",
    "    tmax_in = tmin_in + 1\n",
    "    tmax = events1.loc[tmax_in, \"latency\"]\n",
    "    return tmin, tmax\n",
    "\n",
    "def cut_event_intervals(eeg, tmin, tmax):\n",
    "    # helper function to slice up the eeg data so each slice is associated with one event\n",
    "    eeg_slice = eeg.loc[(eeg[\"time\"] > tmin) & (eeg[\"time\"] < tmax)]\n",
    "    eeg_slice.drop([\"time\", \"sample\"], axis = 1, inplace = True)\n",
    "    return eeg_slice\n",
    "    \n",
    "def build_array(eeg_slice, eeg_slice_length, index, index_list, array_list):\n",
    "    # helper function to build an array out of the eeg slices and pad them out to a standard length\n",
    "    if len(eeg_slice) < eeg_slice_length:\n",
    "        index_list.append(index)\n",
    "        eeg_matrix = eeg_slice.as_matrix()\n",
    "        padded_matrix = np.pad(eeg_matrix, ((0, eeg_slice_length - len(eeg_matrix)), (0,0)),\n",
    "                                   'constant', constant_values=0)\n",
    "        array_list.append(padded_matrix)\n",
    "    return array_list, index_list\n",
    "\n",
    "def one_hot_events(events):\n",
    "    # helper function for one-hot encoding the events\n",
    "    events_list = list(events)\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(events_list)\n",
    "    events_1hot = lb.transform(events_list)\n",
    "    return events_1hot, lb\n",
    "\n",
    "def invert_one_hot(events, lb):\n",
    "    # function for decoding one-hot, binarizer made in one_hot_events\n",
    "    inv_events = lb.inverse_transform(events)\n",
    "    return inv_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_eeg(eeg_data):\n",
    "    # breaks apart an eeg dataframe, scales the eeg readings, and reassmbles it into a dataframe\n",
    "    column_list = eeg_data.columns[1:33]\n",
    "    time = eeg_data['time']\n",
    "    sample = eeg_data['sample']\n",
    "    eeg_array = eeg_data[column_list]\n",
    "    eeg_stnd = scale_data(eeg_array)\n",
    "    eeg_stnd_df = pd.DataFrame(eeg_stnd, index=eeg_data.index, columns=column_list)\n",
    "    eeg_stnd = pd.concat([time, eeg_stnd_df, sample], axis =1)\n",
    "    return eeg_stnd\n",
    "\n",
    "def scale_data(unscaled_data):\n",
    "    # helper function for standardize_eeg, fits a scaler and transforms the data \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(unscaled_data)\n",
    "    scaled_data = scaler.transform(unscaled_data)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is unused code for breaking up the \"nothing happened\" periods of the eeg data \n",
    "# to associate with \"type 0\" events. \n",
    "\n",
    "import math\n",
    "time_steps = 1300\n",
    "\n",
    "def build_zero_events(event_data, time_steps=time_steps):\n",
    "    new_events = build_new_events(event_data, time_steps)\n",
    "    events = zero_events(event_data, new_events)\n",
    "    return events\n",
    "\n",
    "\n",
    "def build_new_events(event_data, time_steps= time_steps):\n",
    "    first_event_time = event_data['latency'].loc[1]\n",
    "    number_new_intervals = math.floor(first_event_time / time_steps)\n",
    "    df = pd.DataFrame(columns=['number', 'latency', 'type', 'duration'],index = range(number_new_intervals) )\n",
    "    latency = 0\n",
    "    for t in range(number_new_intervals):\n",
    "        latency += 1300\n",
    "        df.loc[t].latency = latency\n",
    "        df.loc[t].type = 0\n",
    "    return df\n",
    "\n",
    "def zero_events(event_data, new_events):\n",
    "    events_zeros = event_data[event_data.latency != 1]\n",
    "    events_zeros= new_events.append(events_zeros)\n",
    "    events_zeros = events_zeros.reset_index(drop=True)\n",
    "    events_zeros['number'] = events_zeros.index + 1\n",
    "    return events_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset parameters\n",
    "\n",
    "# define model parameters\n",
    "samples = 2997  # how many trials of eeg data\n",
    "n_features = 14  # how many channels of eeg in each sample\n",
    "time_steps = 100 # how many ms was each sample run for\n",
    "event_types = 2 #len(set(y))  # how many different event types (light, sound, etc) are there # 6 large, 4 smol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c4e12fd0d25b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gesture'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sample_num'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('full_data_shuffle_24_04_2997.csv')\n",
    "data.columns = ['sample_num', str(0), str(1), str(2), str(3), str(4), str(5),str(6),str(7),str(8),str(9),str(10),str(11),str(12),str(13), 'gesture']\n",
    "\n",
    "#data = data.drop(['Unnamed: 0'], axis = 1)\n",
    "d_t = pd.DataFrame(StandardScaler().fit_transform(data.drop(['sample_num', 'gesture'], axis = 1)))\n",
    "d_t['gesture'] = data['gesture']\n",
    "d_t['sample_num'] = data['sample_num']\n",
    "data = d_t\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.shape\n",
    "len(data['sample_num'].unique())\n",
    "\n",
    "#############################\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np_lst = []\n",
    "sample_nums_rand = data['sample_num'].unique()\n",
    "random.shuffle(sample_nums_rand)\n",
    "for n in sample_nums_rand:\n",
    "    sample = data[data['sample_num'] == n].drop(['sample_num', 'gesture'], axis = 1)\n",
    "    np_lst.append(sample.values)\n",
    "    \n",
    "data_reshaped = np.array(np_lst).reshape((2997, 100,1,14))\n",
    "np_lst\n",
    "\n",
    "#################################\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y = data[['gesture', 'sample_num']]\n",
    "l = []\n",
    "for i in sample_nums_rand:\n",
    "    s_y = y[y['sample_num']==i]\n",
    "    if(s_y['gesture'].iloc[0] == 0):\n",
    "        l.append(0)\n",
    "    else:\n",
    "        l.append(1)\n",
    "#print(data['gesture'])      \n",
    "y = to_categorical(np.array(l).reshape((len(l), )))\n",
    "#y = np.array(l).reshape((len(l), ))\n",
    "\n",
    "X =  data_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# use strat. shuffle split to get indices for test and training data \n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=32)\n",
    "sss.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices generated by stratified shuffle split and make the test and training datasets\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2427 samples, validate on 270 samples\n",
      "Epoch 1/100\n",
      " - 17s - loss: 0.6969 - accuracy: 0.5155 - val_loss: 0.6965 - val_accuracy: 0.5444\n",
      "Epoch 2/100\n",
      " - 16s - loss: 0.6893 - accuracy: 0.5039 - val_loss: 0.6988 - val_accuracy: 0.4778\n",
      "Epoch 3/100\n",
      " - 16s - loss: 0.6829 - accuracy: 0.5311 - val_loss: 0.6987 - val_accuracy: 0.5185\n",
      "Epoch 4/100\n",
      " - 16s - loss: 0.6781 - accuracy: 0.5303 - val_loss: 0.6943 - val_accuracy: 0.5074\n",
      "Epoch 5/100\n",
      " - 16s - loss: 0.6726 - accuracy: 0.5422 - val_loss: 0.7058 - val_accuracy: 0.5185\n",
      "Epoch 6/100\n",
      " - 16s - loss: 0.6631 - accuracy: 0.5575 - val_loss: 0.6950 - val_accuracy: 0.5333\n",
      "Epoch 7/100\n",
      " - 16s - loss: 0.6598 - accuracy: 0.5554 - val_loss: 0.6941 - val_accuracy: 0.5185\n",
      "Epoch 8/100\n",
      " - 16s - loss: 0.6537 - accuracy: 0.5690 - val_loss: 0.6693 - val_accuracy: 0.5148\n",
      "Epoch 9/100\n",
      " - 16s - loss: 0.6498 - accuracy: 0.5871 - val_loss: 0.6734 - val_accuracy: 0.5630\n",
      "Epoch 10/100\n",
      " - 16s - loss: 0.6430 - accuracy: 0.5913 - val_loss: 0.6812 - val_accuracy: 0.5185\n",
      "Epoch 11/100\n",
      " - 16s - loss: 0.6409 - accuracy: 0.5826 - val_loss: 0.6678 - val_accuracy: 0.5333\n",
      "Epoch 12/100\n",
      " - 16s - loss: 0.6334 - accuracy: 0.5987 - val_loss: 0.6984 - val_accuracy: 0.5185\n",
      "Epoch 13/100\n",
      " - 16s - loss: 0.6312 - accuracy: 0.5995 - val_loss: 0.6795 - val_accuracy: 0.5444\n",
      "Epoch 14/100\n",
      " - 16s - loss: 0.6215 - accuracy: 0.6024 - val_loss: 0.6686 - val_accuracy: 0.5444\n",
      "Epoch 15/100\n",
      " - 16s - loss: 0.6205 - accuracy: 0.6156 - val_loss: 0.7758 - val_accuracy: 0.5481\n",
      "Epoch 16/100\n",
      " - 16s - loss: 0.6106 - accuracy: 0.6110 - val_loss: 0.6931 - val_accuracy: 0.5926\n",
      "Epoch 17/100\n",
      " - 16s - loss: 0.6047 - accuracy: 0.6238 - val_loss: 0.6964 - val_accuracy: 0.5852\n",
      "Epoch 18/100\n",
      " - 16s - loss: 0.5965 - accuracy: 0.6345 - val_loss: 0.7402 - val_accuracy: 0.5593\n",
      "Epoch 19/100\n",
      " - 16s - loss: 0.6042 - accuracy: 0.6374 - val_loss: 0.6929 - val_accuracy: 0.5593\n",
      "Epoch 20/100\n",
      " - 16s - loss: 0.5926 - accuracy: 0.6489 - val_loss: 0.7155 - val_accuracy: 0.5519\n",
      "Epoch 21/100\n",
      " - 16s - loss: 0.5812 - accuracy: 0.6502 - val_loss: 0.6615 - val_accuracy: 0.5593\n",
      "Epoch 22/100\n",
      " - 16s - loss: 0.5792 - accuracy: 0.6535 - val_loss: 0.6776 - val_accuracy: 0.5963\n",
      "Epoch 23/100\n",
      " - 16s - loss: 0.5798 - accuracy: 0.6531 - val_loss: 0.7306 - val_accuracy: 0.6037\n",
      "Epoch 24/100\n",
      " - 16s - loss: 0.5626 - accuracy: 0.6737 - val_loss: 0.7008 - val_accuracy: 0.6074\n",
      "Epoch 25/100\n",
      " - 16s - loss: 0.5572 - accuracy: 0.6687 - val_loss: 0.7206 - val_accuracy: 0.6037\n",
      "Epoch 26/100\n",
      " - 16s - loss: 0.5661 - accuracy: 0.6671 - val_loss: 0.7108 - val_accuracy: 0.5593\n",
      "Epoch 27/100\n",
      " - 16s - loss: 0.5482 - accuracy: 0.6827 - val_loss: 0.7077 - val_accuracy: 0.5444\n",
      "Epoch 28/100\n",
      " - 16s - loss: 0.5393 - accuracy: 0.6840 - val_loss: 0.7959 - val_accuracy: 0.5185\n",
      "Epoch 29/100\n",
      " - 16s - loss: 0.5247 - accuracy: 0.7058 - val_loss: 0.7431 - val_accuracy: 0.5741\n",
      "Epoch 30/100\n",
      " - 16s - loss: 0.5218 - accuracy: 0.7013 - val_loss: 0.8021 - val_accuracy: 0.5444\n",
      "Epoch 31/100\n",
      " - 16s - loss: 0.5124 - accuracy: 0.7145 - val_loss: 0.7720 - val_accuracy: 0.5926\n",
      "Epoch 32/100\n",
      " - 17s - loss: 0.5087 - accuracy: 0.7194 - val_loss: 0.8076 - val_accuracy: 0.5926\n",
      "Epoch 33/100\n",
      " - 16s - loss: 0.5044 - accuracy: 0.7173 - val_loss: 0.7776 - val_accuracy: 0.5852\n",
      "Epoch 34/100\n",
      " - 16s - loss: 0.4982 - accuracy: 0.7359 - val_loss: 0.8199 - val_accuracy: 0.5926\n",
      "Epoch 35/100\n",
      " - 16s - loss: 0.4862 - accuracy: 0.7355 - val_loss: 0.8205 - val_accuracy: 0.5852\n",
      "Epoch 36/100\n",
      " - 16s - loss: 0.4819 - accuracy: 0.7470 - val_loss: 0.9069 - val_accuracy: 0.5259\n",
      "Epoch 37/100\n",
      " - 16s - loss: 0.4727 - accuracy: 0.7412 - val_loss: 0.8339 - val_accuracy: 0.5407\n",
      "Epoch 38/100\n",
      " - 17s - loss: 0.4618 - accuracy: 0.7573 - val_loss: 0.8712 - val_accuracy: 0.5333\n",
      "Epoch 39/100\n",
      " - 16s - loss: 0.4502 - accuracy: 0.7668 - val_loss: 0.9485 - val_accuracy: 0.5407\n",
      "Epoch 40/100\n",
      " - 17s - loss: 0.4527 - accuracy: 0.7581 - val_loss: 0.8944 - val_accuracy: 0.5593\n",
      "Epoch 41/100\n",
      " - 17s - loss: 0.4373 - accuracy: 0.7721 - val_loss: 0.9021 - val_accuracy: 0.5333\n",
      "Epoch 42/100\n",
      " - 17s - loss: 0.4224 - accuracy: 0.7742 - val_loss: 0.9917 - val_accuracy: 0.5704\n",
      "Epoch 43/100\n",
      " - 17s - loss: 0.4138 - accuracy: 0.7903 - val_loss: 0.9761 - val_accuracy: 0.5519\n",
      "Epoch 44/100\n",
      " - 17s - loss: 0.4239 - accuracy: 0.7878 - val_loss: 1.0287 - val_accuracy: 0.5704\n",
      "Epoch 45/100\n",
      " - 17s - loss: 0.3975 - accuracy: 0.8059 - val_loss: 1.0341 - val_accuracy: 0.5407\n",
      "Epoch 46/100\n",
      " - 17s - loss: 0.4100 - accuracy: 0.8039 - val_loss: 1.0504 - val_accuracy: 0.5444\n",
      "Epoch 47/100\n",
      " - 17s - loss: 0.3925 - accuracy: 0.8022 - val_loss: 1.0757 - val_accuracy: 0.5778\n",
      "Epoch 48/100\n",
      " - 16s - loss: 0.3744 - accuracy: 0.8154 - val_loss: 1.2602 - val_accuracy: 0.5407\n",
      "Epoch 49/100\n",
      " - 17s - loss: 0.3920 - accuracy: 0.8154 - val_loss: 1.0884 - val_accuracy: 0.5704\n",
      "Epoch 50/100\n",
      " - 16s - loss: 0.3585 - accuracy: 0.8298 - val_loss: 1.1065 - val_accuracy: 0.5333\n",
      "Epoch 51/100\n",
      " - 17s - loss: 0.3559 - accuracy: 0.8274 - val_loss: 1.1238 - val_accuracy: 0.5407\n",
      "Epoch 52/100\n",
      " - 16s - loss: 0.3491 - accuracy: 0.8335 - val_loss: 1.2069 - val_accuracy: 0.5630\n",
      "Epoch 53/100\n",
      " - 16s - loss: 0.3288 - accuracy: 0.8525 - val_loss: 1.1190 - val_accuracy: 0.5815\n",
      "Epoch 54/100\n",
      " - 16s - loss: 0.3337 - accuracy: 0.8447 - val_loss: 1.1624 - val_accuracy: 0.5926\n",
      "Epoch 55/100\n",
      " - 16s - loss: 0.3174 - accuracy: 0.8566 - val_loss: 1.1780 - val_accuracy: 0.5593\n",
      "Epoch 56/100\n",
      " - 16s - loss: 0.3092 - accuracy: 0.8550 - val_loss: 1.2752 - val_accuracy: 0.5407\n",
      "Epoch 57/100\n",
      " - 16s - loss: 0.3163 - accuracy: 0.8550 - val_loss: 1.1202 - val_accuracy: 0.5889\n",
      "Epoch 58/100\n",
      " - 16s - loss: 0.2901 - accuracy: 0.8702 - val_loss: 1.2941 - val_accuracy: 0.5407\n",
      "Epoch 59/100\n",
      " - 16s - loss: 0.2817 - accuracy: 0.8735 - val_loss: 1.3943 - val_accuracy: 0.5519\n",
      "Epoch 60/100\n",
      " - 16s - loss: 0.2828 - accuracy: 0.8698 - val_loss: 1.2700 - val_accuracy: 0.5630\n",
      "Epoch 61/100\n",
      " - 16s - loss: 0.2965 - accuracy: 0.8731 - val_loss: 1.2178 - val_accuracy: 0.5852\n",
      "Epoch 62/100\n",
      " - 16s - loss: 0.2667 - accuracy: 0.8888 - val_loss: 1.3345 - val_accuracy: 0.5593\n",
      "Epoch 63/100\n",
      " - 16s - loss: 0.2541 - accuracy: 0.8937 - val_loss: 1.4355 - val_accuracy: 0.5333\n",
      "Epoch 64/100\n",
      " - 17s - loss: 0.2627 - accuracy: 0.8883 - val_loss: 1.4485 - val_accuracy: 0.5593\n",
      "Epoch 65/100\n",
      " - 17s - loss: 0.2403 - accuracy: 0.8982 - val_loss: 1.4554 - val_accuracy: 0.5630\n",
      "Epoch 66/100\n",
      " - 16s - loss: 0.2356 - accuracy: 0.9036 - val_loss: 1.3780 - val_accuracy: 0.5481\n",
      "Epoch 67/100\n",
      " - 16s - loss: 0.2301 - accuracy: 0.9011 - val_loss: 1.4839 - val_accuracy: 0.5259\n",
      "Epoch 68/100\n",
      " - 16s - loss: 0.2189 - accuracy: 0.9089 - val_loss: 1.4040 - val_accuracy: 0.5481\n",
      "Epoch 69/100\n",
      " - 16s - loss: 0.2040 - accuracy: 0.9155 - val_loss: 1.4529 - val_accuracy: 0.5630\n",
      "Epoch 70/100\n",
      " - 16s - loss: 0.1887 - accuracy: 0.9213 - val_loss: 1.6626 - val_accuracy: 0.5407\n",
      "Epoch 71/100\n",
      " - 16s - loss: 0.2258 - accuracy: 0.9056 - val_loss: 1.5467 - val_accuracy: 0.5481\n",
      "Epoch 72/100\n",
      " - 16s - loss: 0.2077 - accuracy: 0.9205 - val_loss: 1.6121 - val_accuracy: 0.5593\n",
      "Epoch 73/100\n",
      " - 16s - loss: 0.2182 - accuracy: 0.9151 - val_loss: 1.6570 - val_accuracy: 0.5222\n",
      "Epoch 74/100\n",
      " - 16s - loss: 0.1868 - accuracy: 0.9279 - val_loss: 1.7310 - val_accuracy: 0.5481\n",
      "Epoch 75/100\n",
      " - 16s - loss: 0.1818 - accuracy: 0.9271 - val_loss: 1.7646 - val_accuracy: 0.5667\n",
      "Epoch 76/100\n",
      " - 16s - loss: 0.1875 - accuracy: 0.9254 - val_loss: 1.7984 - val_accuracy: 0.5185\n",
      "Epoch 77/100\n",
      " - 16s - loss: 0.1618 - accuracy: 0.9365 - val_loss: 1.8102 - val_accuracy: 0.5296\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-206bf670bda9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# code for building an LSTM with 100 neurons and dropout. Runs for 50 epochs\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(time_steps, n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100)) #dramatically worse results\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=100, verbose = 2, validation_split = 0.1)\n",
    "score = model.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.839839494228363, 0.6100000143051147]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.88%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saved model details\n",
    "standardized\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(2, output_dim=256))\n",
    "model.add(LSTM(100, input_shape=(time_steps, n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=50)\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "This model run for 50 epochs had:\n",
    "\n",
    "* binary crossentropy 0.41922928811677918\n",
    "\n",
    "* accuracy 0.8529411764705882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3c493d472eb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#X_train,X_test,y_train,y_test = train_test_total(X, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model, model_dir=\"/model/\", \n\u001b[0m\u001b[0;32m      4\u001b[0m                                         params = {'hidden_layers' : [32, 32], 'num_classes' : 4, 'learning_rate' : 0.001})\n\u001b[0;32m      5\u001b[0m \u001b[0mtensors_to_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"probabilities\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"softmax_tensor\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#X_train,X_test,y_train,y_test = train_test_total(X, y)\n",
    "eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model, model_dir=\"/model/\", \n",
    "                                        params = {'hidden_layers' : [32, 32], 'num_classes' : 4, 'learning_rate' : 0.001})\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=50)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train},\n",
    "    y=y_train,\n",
    "    batch_size=100,\n",
    "    num_epochs=20,\n",
    "    shuffle=True)\n",
    "\n",
    "eeg_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=100,\n",
    "    hooks=[logging_hook])\n",
    "\n",
    "eval_train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train},\n",
    "    y=y_train,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_train_results = eeg_classifier.evaluate(input_fn=eval_train_fn)\n",
    "\n",
    "eval_test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_test_results = eeg_classifier.evaluate(input_fn=eval_test_fn)\n",
    "print('Train results are:',eval_train_results)\n",
    "print('Test results are:',eval_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCI_model(dropout = 0.5, shape = (100, 1, 14), nb_classes = 2): # shape(timestamps, 1, channels)\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=shape))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    #model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,25), padding='same', activation=''))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense((50)))\n",
    "    model.add(tf.keras.layers.Reshape((50,1)))\n",
    "    model.add(tf.keras.layers.LSTM(20, dropout=0.5, recurrent_dropout=0.5, input_shape=(50,1), return_sequences=False))\n",
    "    model.add(tf.keras.layers.Dense(nb_classes, activation='softmax'))\n",
    "    return model\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ppkol\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\ppkol\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2997 samples\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = BCI_model()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "model.fit(X, y,  epochs = 50,   verbose = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
